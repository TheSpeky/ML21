{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Exercises\n",
    "Remember to look at all the exercises. There is plenty of coding to do here and particularly exercise 1 may take some time to run.\n",
    "The exercisess 2-6 cover the basic neural net theory and should be prioritized, in particular be sure to take at look at exercise 3, 4 and 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: SVM Kernel fitting and cross validation\n",
    "\n",
    "Using grid search cross validation and the SVM package find the best classifier svm classifier for the AU digits.\n",
    "You can choose any kernel you want, but we suggest testing Gaussian and or Polynomial Kernels. Besides the hyperparameters for the choosen kernel you should also optimize for the regularization parameter $C$.\n",
    "\n",
    "See here for help on SVM and Grid Search.\n",
    "- [GridSearch](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "- [SVM](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "**The task is**\n",
    "- Implement a cross validation for svm with gaussian and/or polynomial kernel\n",
    "- Find the best parameter settings for the kernels tested.\n",
    "- Train on all data using the best parameters\n",
    "- Test your final classifier on the test set. You should be able to get above 95 percent test accuracy.\n",
    "\n",
    "**Hint: For saving time, subsampling the data set may help i.e. use only 2-3000 data points in the cross validation**\n",
    "\n",
    "We have added a small script that visualizes the data.\n",
    "\n",
    "You can use the same approach to visualize some support vectors if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data (10380, 784)\n",
      "labels shape and type (10380,) int64\n",
      "file not exists - downloading\n",
      "shape of input data (2580, 784)\n",
      "labels shape and type (2580,) int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1152 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os, urllib\n",
    "import matplotlib.pyplot as plt\n",
    "# load au digits\n",
    "def load_digits_train_data():\n",
    "    \"\"\" Load and return the training data \"\"\"\n",
    "    filename = 'auTrain.npz'\n",
    "    if not os.path.exists(filename):\n",
    "        print('file not exists - downloading')\n",
    "        with open(filename, 'wb') as fh:\n",
    "            path = \"http://users-cs.au.dk/jallan/ml/data/{0}\".format(filename)\n",
    "            fh.write(urllib.request.urlopen(path).read())\n",
    "    tmp = np.load('auTrain.npz')\n",
    "    au_digits = tmp['digits']\n",
    "    print('shape of input data', au_digits.shape)\n",
    "    au_labels = np.squeeze(tmp['labels'])\n",
    "    print('labels shape and type', au_labels.shape, au_labels.dtype)\n",
    "    return au_digits, au_labels\n",
    "\n",
    "def load_digits_test_data():\n",
    "    \"\"\" Load and return the test data \"\"\"\n",
    "    filename = 'auTest.npz';\n",
    "    if not os.path.exists(filename):\n",
    "        print('file not exists - downloading')\n",
    "        with open(filename, 'wb') as fh:\n",
    "            path = \"http://users-cs.au.dk/jallan/ml/data/{0}\".format(filename)\n",
    "            fh.write(urllib.request.urlopen(path).read())\n",
    "    tmp = np.load('auTest.npz')\n",
    "    au_digits = tmp['digits']\n",
    "    print('shape of input data', au_digits.shape)\n",
    "    au_labels = np.squeeze(tmp['labels'])\n",
    "    print('labels shape and type', au_labels.shape, au_labels.dtype)\n",
    "    return au_digits, au_labels\n",
    "\n",
    "au_train_images, au_train_labels = load_digits_train_data()\n",
    "au_test_images, au_test_labels = load_digits_test_data()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(au_train_images[i, :].reshape(28, 28), cmap=plt.cm.gray)\n",
    "        ax.set_title('image of a {0}'.format(au_train_labels[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
      "       'param_C', 'param_gamma', 'param_kernel', 'params', 'split0_test_score',\n",
      "       'split1_test_score', 'split2_test_score', 'mean_test_score',\n",
      "       'std_test_score', 'rank_test_score', 'split0_train_score',\n",
      "       'split1_train_score', 'split2_train_score', 'mean_train_score',\n",
      "       'std_train_score'],\n",
      "      dtype='object')\n",
      "Cross Validation Result: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.939333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>3.560133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.939333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>3.264377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.932333</td>\n",
       "      <td>0.993500</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>2.724546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0.924667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>6.884953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>0.924667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>5.865627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.918667</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>5.384207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.898167</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2.133276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.879166</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2.963643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>0.853667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.06004</td>\n",
       "      <td>6.103530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.853667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.06004</td>\n",
       "      <td>7.224793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.845667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.06004</td>\n",
       "      <td>6.014517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0.798667</td>\n",
       "      <td>0.811329</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4.352293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0.707667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.08002</td>\n",
       "      <td>6.931473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13</td>\n",
       "      <td>0.707667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08002</td>\n",
       "      <td>6.773370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0.679667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08002</td>\n",
       "      <td>6.162440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>0.548667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.441802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>0.548667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.832437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>0.496667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.570914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  mean_test_score  mean_train_score  std_test_score  \\\n",
       "7                 1         0.939333          1.000000        0.004011   \n",
       "13                1         0.939333          1.000000        0.004011   \n",
       "1                 3         0.932333          0.993500        0.004811   \n",
       "8                 4         0.924667          1.000000        0.001170   \n",
       "14                4         0.924667          1.000000        0.001170   \n",
       "2                 6         0.918667          0.999000        0.001672   \n",
       "12                7         0.879000          0.898167        0.002658   \n",
       "6                 8         0.866000          0.879166        0.001999   \n",
       "15                9         0.853667          1.000000        0.005515   \n",
       "9                 9         0.853667          1.000000        0.005515   \n",
       "3                11         0.845667          1.000000        0.004983   \n",
       "0                12         0.798667          0.811329        0.005587   \n",
       "10               13         0.707667          1.000000        0.012250   \n",
       "16               13         0.707667          1.000000        0.012250   \n",
       "4                15         0.679667          1.000000        0.015085   \n",
       "11               16         0.548667          1.000000        0.017382   \n",
       "17               16         0.548667          1.000000        0.017382   \n",
       "5                18         0.496667          1.000000        0.013972   \n",
       "\n",
       "    std_train_score param_C param_gamma  mean_fit_time  \n",
       "7          0.000000       5     0.02008       3.560133  \n",
       "13         0.000000      10     0.02008       3.264377  \n",
       "1          0.000403       1     0.02008       2.724546  \n",
       "8          0.000000       5     0.04006       6.884953  \n",
       "14         0.000000      10     0.04006       5.865627  \n",
       "2          0.000408       1     0.04006       5.384207  \n",
       "12         0.000997      10      0.0001       2.133276  \n",
       "6          0.000945       5      0.0001       2.963643  \n",
       "15         0.000000      10     0.06004       6.103530  \n",
       "9          0.000000       5     0.06004       7.224793  \n",
       "3          0.000000       1     0.06004       6.014517  \n",
       "0          0.005413       1      0.0001       4.352293  \n",
       "10         0.000000       5     0.08002       6.931473  \n",
       "16         0.000000      10     0.08002       6.773370  \n",
       "4          0.000000       1     0.08002       6.162440  \n",
       "11         0.000000       5         0.1       6.441802  \n",
       "17         0.000000      10         0.1       7.832437  \n",
       "5          0.000000       1         0.1       6.570914  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "### YOUR CODE HERE - run crossval here\n",
    "\n",
    "def run_svm(X, Y, parameters, df_filter):\n",
    "    clf = GridSearchCV(svm.SVC(decision_function_shape=\"ovr\"), parameters,n_jobs = 4,cv=3, return_train_score=True)\n",
    "    clf.fit(X, Y)\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    print(df.keys())\n",
    "    df = df.filter(df_filter).sort_values('rank_test_score')\n",
    "    print('Cross Validation Result: ')\n",
    "    display(df)\n",
    "    return clf.best_params_\n",
    "\n",
    "rbf_filter = ['rank_test_score', 'mean_test_score', 'mean_train_score', 'std_test_score', \n",
    "              'std_train_score', 'param_C', 'param_gamma', 'mean_fit_time']\n",
    "    \n",
    "rbf_parameters = {'C': [1, 5, 10],\n",
    "                      'gamma': np.linspace(1e-4,1e-1,6),\n",
    "                      'kernel': ['rbf']\n",
    "                 }\n",
    "\n",
    "poly_filter = ['mean_test_score', 'mean_train_score', 'std_test_score',\n",
    "                  'std_train_score', 'param_C', 'param_gamma', 'mean_fit_time']\n",
    "poly_parameters = {'C': [1, 10, 50],\n",
    "                    'degree': [2],\n",
    "                    'gamma': [0.001, 0.01, 0.1, 1, 5],\n",
    "                    'kernel': ['poly']}\n",
    "\n",
    "\n",
    "rp = np.random.permutation(au_train_labels.size)\n",
    "digs = au_train_images[rp,:]\n",
    "labs = au_train_labels[rp]\n",
    "digs = au_train_images[0:3000, :]\n",
    "labs = au_train_labels[0:3000]\n",
    "\n",
    "best_params = run_svm(digs, labs, rbf_parameters, rbf_filter)\n",
    "### END CODE\n",
    "\n",
    "# print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets train on the best parameters found and test on the test data\n",
      "In Sample Accuracy: 1.0\n",
      "Test Sample Accuracy: 0.9736434108527132\n"
     ]
    }
   ],
   "source": [
    "print('Lets train on the best parameters found and test on the test data')\n",
    "### YOUR CODE HERE\n",
    "svc = svm.SVC(**best_params)\n",
    "svc.fit(au_train_images, au_train_labels)\n",
    "print('In Sample Accuracy:', svc.score(au_train_images, au_train_labels))\n",
    "print('Test Sample Accuracy:', svc.score(au_test_images, au_test_labels))\n",
    "### END CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ex 2: Neural Network Design\n",
    "In this exercise you must hand make neural networks that compute simple functions.\n",
    "For the nonlinear transform you can mix them any way you like but you can only use, identity, sign, relu and sigmoid transforms in the neurons.\n",
    "As a hint the only nonlinear transfrom the teacher uses is relu (and identity).\n",
    "You can make the networks as wide and deep as you would like but small networks are sufficient.\n",
    "* Make a network that computes c \\cdot x for any constant c\n",
    "* Make a network that computes xor of inputs $x_1$ and $x_2$. \n",
    "* Make a network that computes max($x_1$,$x_2$)\n",
    "* Make a network that computes $x^2$ - for x in range {2,3,4,5} i.e. x is an integer\n",
    "\n",
    "### SOLUTION MATH HERE\n",
    " - $c \\cdot x$ is trivial. One input neuron x, one output neuron with identity transform, one weight c.\n",
    " - xor of x, y should be equal to $x + y - 2\\textrm{relu}(x+y-1)$. \n",
    " - $\\max(x,y) = \\frac{1}{2} \\textrm{relu} (x-y) + \\frac{1}{2} \\textrm{relu} (y-x) + \\frac{1}{2}(x+y) $\n",
    " - $x^2 = -x + 2 \\sum_{i=1}^x \\mathrm{relu} (x-i) = -x + 2 x(x+1)/2 = x^2$ \n",
    " \n",
    "### END SOLUTION\n",
    "\n",
    "- **Hint 1: It is usually easier to find an easy mathematical expression that solves the problem and then to make a network that implements that**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3: Neural Net Forward Pass - Mini Batch - Vectorized\n",
    "Write a function that computes the forward pass of a (mini-)batch of inputs to a one hidden layer neural net with identity output neurons and relu nonlinearity $\\textrm{relu}(x) = \\max(0, x)$.\n",
    "\n",
    "To be precise, write an algorithm that takes as input a batch of data and computes the output of the neural net on each input point given in the batch.\n",
    "\n",
    "The data batch is given as an $n \\times d$ matrix $X$, where each row is a data point (the input point as a column vector transposed if you follow the LFD books notation).\n",
    "\n",
    "The hidden layer has size $h$, and the output layer has size 1 (for simplicity - easily generalizes).\n",
    "\n",
    "This requires two sets of weights and biases\n",
    "* The weights that map the input data to the input to hidden units. Call that H. The bias weights for this we name $b_1$.\n",
    "\n",
    "* The weights that map the output of the hidden units to the output. Call that O. The bias weights for this we name $b_2$.\n",
    "\n",
    "We organize the weighs in matrices $(W_1, W_2)$ and vectors $(b_1,b_2)$ as follows:\n",
    "* The $i'th$ column of $W_1$ are the weights we multiply with the input data to get the input hidden node $i$. The size of $W_1$ is $d \\times h$\n",
    "* The bias $b_1$ is a vector of size h, the i'th entry the bias to hidden neuron $i$.\n",
    "* The $i'th$ column of $W_2$ are the weights we multiply with the hidden layer activations to get the input to the i'th output node. $W_2$ is a $h \\times \\textrm{output_size}$ matrix\n",
    "* The bias $b_2$ is a vector of size output_size \n",
    "\n",
    "**Task:** In the cell below (partially) complete the neural net class\n",
    "- Implement the predict function of the neural net\n",
    "- Implement the score function (least squares $\\frac{1}{n} \\sum_i (\\textrm{nn}(x_i) - y_i)^2$\n",
    "\n",
    "The actual error we get is random since we just set random weights.\n",
    "\n",
    "**Hint**. What is the matrix product $X W_1$ where $X$ is the data matrix with the mini-batch if data where each input is a row and $W_1$ is the hidden layer weight matrix just defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net initialized with random values\n",
      "shape of nn_out (10, 1)\n",
      "least squares error:  9.298166958531457\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "class NN():\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_size=1):\n",
    "        self.W1 = np.random.rand(input_dim, hidden_size)\n",
    "        self.b1 = np.random.rand(1, hidden_size)\n",
    "        self.W2 = np.random.rand(hidden_size, output_size)\n",
    "        self.b2 = np.random.rand(1, output_size)\n",
    "        print('Neural net initialized with random values')\n",
    "        \n",
    "    def predict(self, X):    \n",
    "        \"\"\" Evaluate the network on given data batch \n",
    "        \n",
    "        Args:\n",
    "        X: np.array shape (n, d)  Each row is a data point\n",
    "        \n",
    "        Output:\n",
    "        pred: np.array shape (n, 1) output of network on each input point\n",
    "        \"\"\"\n",
    "        # compute the following values\n",
    "        pred = None # the output of neural net n x 1\n",
    "    \n",
    "        ### YOUR CODE HERE\n",
    "        h_in = X @ self.W1 + self.b1\n",
    "        h = np.maximum(h_in, 0)\n",
    "        out_in = h @ self.W2 + self.b2\n",
    "        pred = out_in\n",
    "        ### END CODE\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute least squares loss (1/n sum (nn(xi) - y)^2)\n",
    "        \n",
    "          X: torch.tensor shape (n, d) - Data\n",
    "          y: torch.tensor shape (n, 1) - Targets\n",
    "\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        ### YOUR CODE HERE\n",
    "        pred = self.predict(X)\n",
    "        assert pred.shape == y.shape, (pred.shape, y.shape)\n",
    "        loss = np.mean((pred - y)**2)\n",
    "        ### END CODE\n",
    "        return loss\n",
    "        \n",
    "# Change to reasonable data we can plot after training\n",
    "# np.random.RandomState(42)\n",
    "input_dim = 3\n",
    "hidden_size = 8\n",
    "X = np.random.rand(10, input_dim)\n",
    "y = np.random.rand(10, 1)\n",
    "my_net = NN(input_dim=input_dim, hidden_size=hidden_size)\n",
    "\n",
    "nn_out = my_net.predict(X)\n",
    "print('shape of nn_out', nn_out.shape) # should be n x 1\n",
    "print('least squares error: ', my_net.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Backpropagation\n",
    "\n",
    "Consider the simple neural net \n",
    "$$\n",
    "\\textrm{nn}(x_1,x_2) = w_3 \\cdot \\textrm{relu}(w_1 \\cdot x_1 + w_2 \\cdot x_2)\n",
    "$$\n",
    "where $\\textrm{relu}(x) = \\max(x,0)$ and $w_1, w_2, w_3$ are the weights of the network\n",
    "\n",
    "We only have one input point $x=(x_1,x_2) = (3.0, 1.0)$ with target value $y=9.0$\n",
    "\n",
    "Consider the error \n",
    "$$\n",
    "e=(y-\\textrm{nn}(x))^2\n",
    "$$\n",
    "We need to minimize the error over $w_1, w_2, w_3$ and will do that using the gradients of $e$ relative to $w_1, w_2, w_3$.\n",
    "Initialize $w_1=1,w_2=2, w_3=1$\n",
    "\n",
    "Draw the computational graph for $e$ and run the forward pass to compute the error, and then run the backwards pass to compute the derivative of $w_1,w_2, w_3$ on the fixed input $x,y$\n",
    "\n",
    "Write the python code that performs the forward and backwards pass below and evaluate the cost and the gradient\n",
    "using notation similar to\n",
    "http://cs231n.github.io/optimization-2/\n",
    "\n",
    "Print intermediate steps in both the forward and the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the python forward and backward pass here\n",
      "z1 3.0\n",
      "z2 2.0\n",
      "hin 5.0\n",
      "hout 5.0\n",
      "pred 5.0\n",
      "diff -4.0\n",
      "ls error 16.0\n",
      "dz 3.0 1.0\n",
      "d_diff -8.0\n",
      "d_pred -8.0\n",
      "d_hout -8.0\n",
      "d_hin -8.0\n",
      "d_w3: -40.0\n",
      "d_w2: -8.0\n",
      "d_w1: -24.0\n"
     ]
    }
   ],
   "source": [
    "print('Do the python forward and backward pass here')\n",
    "### YOUR CODE HERE\n",
    "x1 = 3.0\n",
    "x2 = 1.0 \n",
    "y = 9.\n",
    "w1 = 1.0\n",
    "w2 = 2.0\n",
    "w3 = 1.0\n",
    "# Long Forward Pass\n",
    "z1 = w1 * x1\n",
    "z2 = w2 * x2\n",
    "hin = z1 + z2\n",
    "hout = max(hin, 0)\n",
    "pred = w3 * hout\n",
    "diff = pred - y\n",
    "e = diff**2\n",
    "print('z1', z1)\n",
    "print('z2', z2)\n",
    "print('hin', hin)\n",
    "print('hout', hout)\n",
    "print('pred', pred)\n",
    "print('diff', diff)\n",
    "print('ls error', e)\n",
    "\n",
    "# backwards pass\n",
    "de_diff = 2 * diff\n",
    "ddiff_pred = 1\n",
    "de_pred = de_diff * ddiff_pred\n",
    "dpred_w3 = hout\n",
    "de_w3 = de_pred * dpred_w3 # dout_w3 = rs\n",
    "dpred_hout = w3\n",
    "de_hout = de_pred * dpred_hout\n",
    "dhout_hin = hin > 0\n",
    "de_hin = de_hout * dhout_hin\n",
    "dhin_z1 = 1\n",
    "dhin_z2 = 1\n",
    "de_z1 = de_hin * dhin_z1\n",
    "de_z2 = de_hin * dhin_z2\n",
    "dz1_w1 = x1\n",
    "dz2_w2 = x2\n",
    "print('dz', dz1_w1, dz2_w2)\n",
    "de_w1 = de_z1 * dz1_w1\n",
    "de_w2 = de_z2 * dz2_w2\n",
    "print('d_diff', de_diff)\n",
    "print('d_pred', de_diff)\n",
    "print('d_hout', de_diff)\n",
    "print('d_hin', de_diff)\n",
    "print('d_w3:', de_w3)\n",
    "print('d_w2:', de_w2)\n",
    "print('d_w1:', de_w1)\n",
    "\n",
    "### END CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Automated Backpropagation using pytorch\n",
    "In this exercise we will check our results from the previous exercise using pytorch.\n",
    "\n",
    "For this we only need to code the forward pass and let automatic differentation take care of the rest!\n",
    "\n",
    "**Task:** Write the forward pass in the cell below and use automatic differentation to test your answer from above.\n",
    "\n",
    "Use x.retain_grad() to keep the gradient of any intermediate computation used in the forward pass to compare with above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass\n",
      "least squares error tensor([[16.]], grad_fn=<PowBackward0>)\n",
      "d_diff tensor([[-8.]])\n",
      "d_pred tensor([[-8.]])\n",
      "d_hout tensor([[-8.]])\n",
      "d_hin tensor([[-8.]])\n",
      "d_W2 (d_w3) tensor([[-40.]])\n",
      "d_W1 [d_w1, d_w2] tensor([[-24.],\n",
      "        [ -8.]])\n",
      "Lets show the computation graph\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"196pt\" height=\"388pt\"\n",
       " viewBox=\"0.00 0.00 196.32 388.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 384)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-384 192.3232,-384 192.3232,4 -4,4\"/>\n",
       "<!-- 4590047864 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4590047864</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"155.3076,-20 63.3389,-20 63.3389,0 155.3076,0 155.3076,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.3232\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 4590047976 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4590047976</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"157.4723,-76 61.1742,-76 61.1742,-56 157.4723,-56 157.4723,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.3232\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ThSubBackward</text>\n",
       "</g>\n",
       "<!-- 4590047976&#45;&gt;4590047864 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4590047976&#45;&gt;4590047864</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M109.3232,-55.9883C109.3232,-48.9098 109.3232,-39.1714 109.3232,-30.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.8233,-30.3038 109.3232,-20.3039 105.8233,-30.3039 112.8233,-30.3038\"/>\n",
       "</g>\n",
       "<!-- 4590047696 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4590047696</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"160.1245,-132 58.522,-132 58.522,-112 160.1245,-112 160.1245,-132\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.3232\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ExpandBackward</text>\n",
       "</g>\n",
       "<!-- 4590047696&#45;&gt;4590047976 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4590047696&#45;&gt;4590047976</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M109.3232,-111.9883C109.3232,-104.9098 109.3232,-95.1714 109.3232,-86.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.8233,-86.3038 109.3232,-76.3039 105.8233,-86.3039 112.8233,-86.3038\"/>\n",
       "</g>\n",
       "<!-- 4590048200 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4590048200</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"158.3018,-188 60.3447,-188 60.3447,-168 158.3018,-168 158.3018,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.3232\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ThMulBackward</text>\n",
       "</g>\n",
       "<!-- 4590048200&#45;&gt;4590047696 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4590048200&#45;&gt;4590047696</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M109.3232,-167.9883C109.3232,-160.9098 109.3232,-151.1714 109.3232,-142.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.8233,-142.3038 109.3232,-132.3039 105.8233,-142.3039 112.8233,-142.3038\"/>\n",
       "</g>\n",
       "<!-- 4590048312 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4590048312</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"116.4703,-250 .1762,-250 .1762,-230 116.4703,-230 116.4703,-250\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.3232\" y=\"-236.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ClampMinBackward</text>\n",
       "</g>\n",
       "<!-- 4590048312&#45;&gt;4590048200 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4590048312&#45;&gt;4590048200</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M66.7449,-229.762C74.32,-220.553 85.6165,-206.82 94.6705,-195.8131\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.4086,-197.9939 101.0583,-188.0475 92.0026,-193.547 97.4086,-197.9939\"/>\n",
       "</g>\n",
       "<!-- 4590048480 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4590048480</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"100.4724,-312 16.1741,-312 16.1741,-292 100.4724,-292 100.4724,-312\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.3232\" y=\"-298.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 4590048480&#45;&gt;4590048312 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4590048480&#45;&gt;4590048312</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M58.3232,-291.762C58.3232,-283.185 58.3232,-270.6836 58.3232,-260.1154\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.8233,-260.0475 58.3232,-250.0475 54.8233,-260.0476 61.8233,-260.0475\"/>\n",
       "</g>\n",
       "<!-- 4590048592 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4590048592</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"85.3232,-380 31.3232,-380 31.3232,-348 85.3232,-348 85.3232,-380\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.3232\" y=\"-366.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">W1</text>\n",
       "<text text-anchor=\"middle\" x=\"58.3232\" y=\"-354.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2, 1)</text>\n",
       "</g>\n",
       "<!-- 4590048592&#45;&gt;4590048480 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4590048592&#45;&gt;4590048480</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M58.3232,-347.7102C58.3232,-340.0144 58.3232,-330.744 58.3232,-322.5691\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.8233,-322.3512 58.3232,-312.3512 54.8233,-322.3513 61.8233,-322.3512\"/>\n",
       "</g>\n",
       "<!-- 4590048368 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4590048368</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"188.3232,-256 134.3232,-256 134.3232,-224 188.3232,-224 188.3232,-256\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.3232\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">W2</text>\n",
       "<text text-anchor=\"middle\" x=\"161.3232\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1, 1)</text>\n",
       "</g>\n",
       "<!-- 4590048368&#45;&gt;4590048200 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4590048368&#45;&gt;4590048200</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M147.6609,-223.7102C140.5462,-215.2273 131.8269,-204.8313 124.5023,-196.0981\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.1127,-193.764 118.0049,-188.3512 121.7494,-198.2623 127.1127,-193.764\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x111973f28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot # install this package \n",
    "\n",
    "### YOUR CODE HERE \n",
    "x = torch.tensor([[3., 1.]])\n",
    "y = torch.tensor([9.])\n",
    "W1 = torch.tensor([[1.], [2.]], requires_grad=True)\n",
    "W2 = torch.tensor([[1.]], requires_grad=True)\n",
    "hin = x @ W1\n",
    "hout = hin.clamp(min=0)\n",
    "pred = hout * W2\n",
    "diff = pred - y\n",
    "loss = diff**2\n",
    "print('forward pass')\n",
    "print('least squares error', loss)\n",
    "hin.retain_grad()\n",
    "hout.retain_grad()\n",
    "pred.retain_grad()\n",
    "diff.retain_grad()\n",
    "loss.backward()\n",
    "print('d_diff', diff.grad)\n",
    "print('d_pred', pred.grad)\n",
    "print('d_hout', hout.grad)\n",
    "print('d_hin', hin.grad)\n",
    "print('d_W2 (d_w3)', W2.grad)\n",
    "print('d_W1 [d_w1, d_w2]', W1.grad)\n",
    "\n",
    "### END CODE\n",
    "# print the graph - change naming appropriately\n",
    "print('Lets show the computation graph')\n",
    "make_dot(loss, params={'W1': W1, 'W2': W2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 6: Derivative/Jacobian of applying function elementwise to vector \n",
    "In this exercise we will consider the problem of computing the Jacobian/Gradient of mapping a vector of size $n$ to a vector of size $n$ by applying a function $f$ to each entry in the input.\n",
    "\n",
    "Let $f$ be a smooth function from $\\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Let $F: \\mathbb{R}^d \\rightarrow \\mathbb{R^d}$ defined as $F(v) = \\left[f(v_1), \\dots, f(v_d)\\right]$\n",
    "\n",
    "- Show that the Jacobian $J_F = D_f$, the $d \\times d$ diagonal matrix where the $i$'th diagonal entry is $f'(v_i)$.\n",
    "- If $f(x) = 1/(1+e^{-x})$ i.e. the logistic sigmoid function then how does the Diagonal matrix look\n",
    "- If $f(x) = \\max(0, x)$ i.e. relu how does the Diagonal Matrix look\n",
    "- Consider a neural net backpropagation step and let hout be the output of applying F to the vector hin and assume we have computed the vector $\\frac{\\partial L}{\\partial \\textrm{hout}}$ ($1 \\times h$)\n",
    "    and wish to compute  $\\frac{\\partial L}{\\partial \\textrm{hin}}$ (also $1\\times h)$. Write the pyhton code that acvieves that below\n",
    "    \n",
    "## SOLUTION MATH HERE\n",
    " The jacobian is a $d \\times d$ matrix\n",
    " \n",
    " The derivative of $f_i = f(v_i)$ after $v_j$ is zero if $i != j$. So we get a diagonal matrix.\n",
    " \n",
    " If i==j then we need the derivative of $f(v_i)$ as a function of v_i which is $f'(v_i)$.\n",
    " \n",
    " For sigmoid we get $D_{i,i} = f(x_i)(1-f(x_i))$ \n",
    " \n",
    " For relu we get $D_{i, i} = x_i > 0$ (indicator variables)\n",
    "## END SOLUTION\n",
    "\n",
    "**hint:** There may be a more efficient way than actually creating the diagonal matrix and multiplying on the backpropagated derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d_hout = np.random.rand(1, 42)\n",
    "def relu_grad(d_hout, hin):\n",
    "    d_hin = None\n",
    "    ### YOUR CODE HERE\n",
    "    d_hin = d_hout.copy()\n",
    "    d_hin[hin<0] = 0\n",
    "    #d_hin = d_hin * (hin<0)\n",
    "    \n",
    "    ### END CODE\n",
    "    return d_hin\n",
    "\n",
    "def sigmoid_grad(d_hout, hin):\n",
    "    d_hin = None\n",
    "    ### YOUR CODE HERE\n",
    "    def sigmoid(x):\n",
    "        return 1./(1+np.exp(-x))\n",
    "    d_hin =  d_hout * (sigmoid(hin)*(1-sigmoid(hin))) # entrywise multiplication\n",
    "    ### END CODE\n",
    "    return d_hin\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 7: Neural Nets by automatic differentation in pytorch\n",
    "Before we introduce our selves to the idiomatic way of writing neural net code in pytorch we will implement a pair of basic basic neural nets and train it using just automatic differentation and the optim module introduced last week.\n",
    "\n",
    "**The task is:**\n",
    "\n",
    "Using pytorch implement Linear Regression with a weight decay (Ridge Regression) using Gradient Descent as Learning Algorithm and implement a one hidden layer neural net with relu activation for regression with Least Squares Cost and gradient descent learning algorithm.\n",
    "\n",
    "\n",
    "1. Implement Linear Regression with Gradient Descent and test on the Boston data set for house prices (see cell below)\n",
    "   Linear regression is simply a neural net with no hidden layer and one output neuron.\n",
    "2. Implement  implement a standard one hidden layer neural net for regression. That means identity output activation and least squares error. For nonlineariry use relu. See cell two below\n",
    "\n",
    "For both steps complete the following functions below in the respective cells for Linear Regression and Neural Nets\n",
    "- cost - compute the least squares cost of the network on data and return the pytorch tensor of that\n",
    "- fit - train 100 steps og gradient descent using optim package - find a good learning rate your self\n",
    "\n",
    "We test your implementation on a standard regression data set for house prices and compare to the sklearn build in Ridge Regression. \n",
    "We only consider in sample error, which is of course not what we care about in the real world!.\n",
    "\n",
    "Your Linear Regression implementation should get close to the sklearn Ridge Regression implementation that we have included\n",
    "\n",
    "For the Neural Net you should do better (in sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Load and Prepare Data *****\n",
      "\n",
      " ***** Test Sklearn Ridge Regression for Comparison *****\n",
      "Ridge Regression Score: 21.897789962737757\n",
      "\n",
      " ***** Make data to torch tensors *****\n",
      "\n",
      " ***** Run Torch Linear Regression Gradient Descent *****\n",
      "epoch: 0 least squares (regularized loss) 592.1470947265625\n",
      "epoch: 10 least squares (regularized loss) 32.519954681396484\n",
      "epoch: 20 least squares (regularized loss) 26.205495834350586\n",
      "epoch: 30 least squares (regularized loss) 25.997793197631836\n",
      "epoch: 40 least squares (regularized loss) 25.946170806884766\n",
      "epoch: 50 least squares (regularized loss) 25.92633056640625\n",
      "epoch: 60 least squares (regularized loss) 25.91810417175293\n",
      "epoch: 70 least squares (regularized loss) 25.91451072692871\n",
      "epoch: 80 least squares (regularized loss) 25.91286277770996\n",
      "epoch: 90 least squares (regularized loss) 25.912076950073242\n",
      "pytorch Linear Regression Regression least squares score: 22.679466247558594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "\n",
    "print('*'*5, 'Load and Prepare Data', '*'*5)\n",
    "dataset = load_boston()\n",
    "# print('dataset', dataset)\n",
    "X, y = dataset.data, dataset.target\n",
    "X = (X - X.mean(axis=0))/(X.std(axis=0))\n",
    "#print('data stats', X.shape, X.mean(axis=0), X.std(axis=0))\n",
    "ridge=linear_model.Ridge(alpha=0.1, fit_intercept=True)\n",
    "ridge.fit(X, y)\n",
    "# print(ridge.coef_, ridge.intercept_)\n",
    "print('\\n', '*'*5, 'Test Sklearn Ridge Regression for Comparison', '*'*5)\n",
    "print('Ridge Regression Score:', ((ridge.predict(X)-y)**2).mean())\n",
    "\n",
    "print('\\n', '*'*5, 'Make data to torch tensors', '*'*5)\n",
    "tX = torch.from_numpy(X).float()\n",
    "ty = torch.from_numpy(y).float().view(-1, 1)\n",
    "\n",
    "\n",
    "class LR():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def cost(self, X, y, w, b, c=0):\n",
    "        \"\"\" Compute Regularized Least Squares Loss\n",
    "        \n",
    "          X: torch.tensor shape (n, d) - Data\n",
    "          y: torch.tensor shape (n, 1) - Targets\n",
    "          w: torch.tensor shape (d ,1) - weights\n",
    "          b: torch.tensor shape (1 ,1) - bias weight\n",
    "          c: scalar, ridge regression weight decay parameter (lambda)\n",
    "          \n",
    "          returns (regularized) cost tensor        \n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        ### YOUR CODE HERE\n",
    "        pred = X @ w + b \n",
    "        loss = torch.mean((pred-y)**2) \n",
    "        reg_loss = torch.sum(w**2)\n",
    "        ### END CODE\n",
    "        return loss + c * reg_loss\n",
    "    \n",
    "    def fit(self, X, y, c=0):\n",
    "        \"\"\" GD Learning Algorithm for Ridge Regression with pytorch\n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         c: ridge regression weight decay parameter (lambda)\n",
    "        \"\"\"\n",
    "        w = torch.zeros(X.shape[1], 1, requires_grad=True)\n",
    "        b = torch.zeros(1, 1, requires_grad=True)\n",
    "        ### YOUR CODE HERE use optim.SGD, remember to call zero_grad before each iteration to clear accumulated gradients\n",
    "        sgd = optim.SGD(params={w, b}, lr=0.1)\n",
    "        for i in range(100):\n",
    "            sgd.zero_grad()\n",
    "            # pred = X @ w + b\n",
    "            # loss = torch.mean((pred-y)**2) \n",
    "            loss = self.cost(X, y, w, b, c=c)\n",
    "            if i % 10 == 0:\n",
    "                print('epoch:', i, 'least squares (regularized loss)', loss.item())\n",
    "            loss.backward()\n",
    "            #print('W grad', w.grad)\n",
    "            # tg = torch.transpose(X, 1, 0) @ (X@w -y )            \n",
    "            #print('true grad', tg.shape)\n",
    "            sgd.step()\n",
    "        ### END CODE\n",
    "        self.w = w.clone()\n",
    "        self.b = b.clone()\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute least squares cost for model \n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         \n",
    "        returns least squares score of model on data X with targets y\n",
    "        \"\"\"\n",
    "        score = None\n",
    "        ### YOUR CODE HERE\n",
    "        score = self.cost(X, y, self.w, self.b, c=0)\n",
    "        ### END CODE\n",
    "        return score\n",
    "\n",
    "print('\\n', '*'*5, 'Run Torch Linear Regression Gradient Descent', '*'*5)\n",
    "\n",
    "tlr = LR()\n",
    "tlr.fit(tX, ty, 0.1)\n",
    "print('pytorch Linear Regression Regression least squares score:', tlr.score(tX, ty).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can i acces that SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0 nn least squares loss 658.3987426757812\n",
      "epoch: 10 nn least squares loss 34.00477981567383\n",
      "epoch: 20 nn least squares loss 24.214521408081055\n",
      "epoch: 30 nn least squares loss 21.22365951538086\n",
      "epoch: 40 nn least squares loss 19.591136932373047\n",
      "epoch: 50 nn least squares loss 18.43902587890625\n",
      "epoch: 60 nn least squares loss 17.50029182434082\n",
      "epoch: 70 nn least squares loss 16.732383728027344\n",
      "epoch: 80 nn least squares loss 16.068532943725586\n",
      "epoch: 90 nn least squares loss 15.45358657836914\n",
      "pytorch Neural Net Regression score: 14.915559768676758\n"
     ]
    }
   ],
   "source": [
    "class NN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def cost(self, X, y, W1, b1, W2, b2, c=0):\n",
    "    \"\"\" Compute (Regularized) Least Squares Loss of neural net\n",
    "        \n",
    "          X: torch.tensor shape (n, d) - Data\n",
    "          y: torch.tensor shape (n, 1) - Targets\n",
    "          W1: torch.tensor shape (d, h) - weights\n",
    "          b1: torch.tensor shape (1, h) - bias weight\n",
    "          W2: torch.tensor shape (d, 1) - weights\n",
    "          b2: torch.tensor shape (1, 1) - bias weight\n",
    "          c: ridge regression weight decay parameter (lambda)\n",
    "    \n",
    "        returns (regularized) cost tensor\n",
    "        \"\"\"\n",
    "   \n",
    "        loss = None\n",
    "        ### YOUR CODE HERE\n",
    "        hin = X @ W1 + b1\n",
    "        hout = hin.clamp(min=0)\n",
    "        pred = hout @ W2 + b2\n",
    "        loss = torch.mean((pred-y)**2) \n",
    "        reg_loss = c * torch.sum(W1**2) + c * torch.sum(W2**2)\n",
    "        ### END CODE\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, hidden_size=32, c=0.01):   \n",
    "        \"\"\" GD Learning Algorithm for Ridge Regression with pytorch\n",
    "        \n",
    "         Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         hidden_size: int \n",
    "         c: float weight decay parameter (lambda)\n",
    "        \"\"\"\n",
    "        input_dim = X.shape[1]        \n",
    "        W1 = torch.randn(input_dim, hidden_size, requires_grad=True)\n",
    "        b1 = torch.randn(1, hidden_size, requires_grad=True)\n",
    "        W2 = torch.randn(hidden_size, 1, requires_grad=True)\n",
    "        b2 = torch.randn(1, 1, requires_grad=True)\n",
    "        ### YOUR CODE HERE\n",
    "        sgd = optim.SGD(params={W1, W2, b1, b2}, lr=0.01)\n",
    "        for i in range(100):\n",
    "            sgd.zero_grad()\n",
    "            loss = self.cost(X, y, W1, b1, W2, b2, c=c)\n",
    "            if i % 10 == 0:\n",
    "                print('epoch:', i, 'nn least squares loss', loss.item())\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "        ### END CODE\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute least squares cost for model \n",
    "        \n",
    "        Args:\n",
    "         X: torch.tensor shape (n, d)\n",
    "         y: torch.tensor shape (n, 1)\n",
    "         \n",
    "        returns least squares score of model on data X with targets y\n",
    "        \"\"\"\n",
    "        score = None\n",
    "        ### YOUR CODE HERE\n",
    "        score = self.cost(X, y, self.W1, self.b1, self.W2, self.b2, c=0)\n",
    "        ### END CODE\n",
    "        return score\n",
    "\n",
    "\n",
    "net = NN()\n",
    "net.fit(tX, ty, hidden_size=16, c=0.01)\n",
    "print('pytorch Neural Net Regression least squares score:', net.score(tX, ty).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex: 8 Vectorized Backpropagation (may be  hard)\n",
    "In this exercise you are to compute the derivative of a loss function using vectorized derivatives.\n",
    "\n",
    "Define (data) matrices\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    x_1= [1., 1.]^\\intercal \\\\\n",
    "    x_2= [-1., 5.]^\\intercal \\\\\n",
    "    x_3 = [-1., -1.]]^\\intercal\n",
    "    \\end{bmatrix} \n",
    "    ,\\quad\n",
    " y = [1., 1., 1.]^\\intercal \n",
    "$$\n",
    "where $X$ is a $3 \\times 2$ matrix and $y$ a $3 \\times 1$ matrix.\n",
    "\n",
    "Furthermore define\n",
    "$$\n",
    "W = [2., 3.]^\\intercal, \\quad\n",
    "b = [-2] \n",
    "$$\n",
    "where $W$ is a matrix of shape $2 \\times 1$.\n",
    "and $b$ is a $1 \\times 1$ matrix (or scalar)\n",
    "\n",
    "\n",
    "Now define our simple neural net\n",
    "$$ \n",
    "\\textrm{nn}(x) = \\textrm{relu}(X W) + b\n",
    "$$\n",
    "and let \n",
    "$$\n",
    "L(X, y) = - \\sum_{i=1}^3 (y_i \\lg \\sigma(\\textrm{nn}(x_i)) + (1-y)\\lg(1-\\sigma(\\textrm{nn}(x_i))))\n",
    "$$\n",
    "denote the cross entropy loss. Notice that the loss applies the sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "\n",
    "\n",
    "Compute the derivative of the loss L in regards to every input value in $X$ $\\frac{\\partial \\textrm{loss} }{\\partial X}$ at input point $X$.\n",
    "That means you must derive\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{i, j}}(X)\n",
    "$$\n",
    "for all $i, j$.\n",
    "\n",
    "Write out the formulas to compute these derivatives using backpropagation and try to write them in vectoried notation. \n",
    "That means you need derivatives of all the steps on the way.\n",
    "\n",
    "Test your formulas by using pytorch below.\n",
    "\n",
    "\n",
    "## SOLUTION MATH HERE\n",
    "Define $\\textrm{hin} = (X@W) + b$, hout=relu(hin) as in the code below\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\lg \\sigma(z)} {\\partial z} = \\frac{1}{\\sigma(z)} \\sigma(z) (1-\\sigma(z) = (1-\\sigma(z))\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "d_{L, \\textrm{nn}(x)} = d_\\textrm{hout} = \\left[\\frac{\\partial L}{\\textrm{nn}(x)[i]},  i=1,2,3 \\right]= \\left[-(1-\\sigma(\\textrm{hout}[i])), i=1,2,3 \\right] = -(1-\\sigma(\\textrm{hout}))\n",
    "$$\n",
    "That was derivtive of cost relative to output of neural net hout. And it should be a $1 \\times 3$ vector.\n",
    "Now\n",
    "$$\n",
    "d_{hout, hin} = D_{relu}\n",
    "$$\n",
    "the diagonal matrix with relu derivative in diagonal which is 1 except entries where hin < 0\n",
    "Thus\n",
    "$$\n",
    "d_{L, hin} = d_\\textrm{hin} = -(1-\\sigma(hout)) D_{relu}\n",
    "$$\n",
    "Also a $1 \\times 3$ vector in this case.\n",
    "\n",
    "Now the final step use the beautifull rule from the link and we get\n",
    "$$\n",
    "\\frac{\\partial L }{\\partial X} = d_{L, hin}  W^\\intercal\n",
    "$$\n",
    "\n",
    "Solution in code is below the pytorch code\n",
    "\n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "**Hints:**\n",
    "- Notice that $y$ is always 1 - which means for simplicity we can ignore the symmetric case when y = 0 in all computations \n",
    "- Compute/Define store hin as input to relu and hout as output of relu (the output of the net) compute derivative of loss relative to hin and hout in backprop\n",
    "- As first step compute $d_{L, \\textrm{nn}(x)} = \\frac{\\partial L}{\\partial \\textrm{hout}}(x)$ (a $1 \\times 3$ vector) (for simplicity remeber that y is always 1 in our case)\n",
    "- Use Rule Above for Relu to compute vector of derivatives of L relative to hin (input to relu) \n",
    "- Use the matrix product rule from http://cs231n.github.io/optimization-2/ (the very end) to go from the derivative relative to hin to derivative relative to X \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes X, y, W, b torch.Size([3, 2]) torch.Size([3, 1]) torch.Size([2, 1]) torch.Size([1, 1])\n",
      "hin tensor([[ 3.],\n",
      "        [11.],\n",
      "        [-7.]], grad_fn=<ThAddBackward>)\n",
      "hout tensor([[ 3.],\n",
      "        [11.],\n",
      "        [ 0.]], grad_fn=<ClampMinBackward>)\n",
      "lg shape torch.Size([3, 1])\n",
      "shapes torch.Size([3, 1]) torch.Size([3, 1]) torch.Size([3, 1]) torch.Size([3, 1])\n",
      "loss tensor(0.7418, grad_fn=<NegBackward>)\n",
      "Grad y tensor([[ -3.0000],\n",
      "        [-11.0007],\n",
      "        [  0.0000]])\n",
      "Grad X\n",
      " tensor([[-0.0949, -0.1423],\n",
      "        [-0.0000, -0.0001],\n",
      "        [ 0.0000,  0.0000]])\n",
      "hout grad\n",
      " tensor([[-0.0474],\n",
      "        [-0.0000],\n",
      "        [-0.5000]])\n",
      "hin grad\n",
      " tensor([[-0.0474],\n",
      "        [-0.0000],\n",
      "        [-0.0000]])\n",
      "The hard way\n",
      "d_hout.shape torch.Size([3, 1])\n",
      "d_hout tensor([[-0.0474],\n",
      "        [-0.0000],\n",
      "        [-0.5000]], grad_fn=<NegBackward>)\n",
      "d_hin tensor([[-0.0474],\n",
      "        [-0.0000],\n",
      "        [-0.0000]], grad_fn=<ThMulBackward>)\n",
      "d_X:\n",
      " tensor([[-0.0949, -0.1423],\n",
      "        [-0.0000, -0.0001],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"312pt\" height=\"724pt\"\n",
       " viewBox=\"0.00 0.00 311.64 724.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 720)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-720 307.6445,-720 307.6445,4 -4,4\"/>\n",
       "<!-- 4831703784 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>4831703784</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"145.1268,-20 60.8518,-20 60.8518,0 145.1268,0 145.1268,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.9893\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">NegBackward</text>\n",
       "</g>\n",
       "<!-- 4831703728 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4831703728</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"149.1423,-76 56.8363,-76 56.8363,-56 149.1423,-56 149.1423,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.9893\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SumBackward0</text>\n",
       "</g>\n",
       "<!-- 4831703728&#45;&gt;4831703784 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4831703728&#45;&gt;4831703784</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.9893,-55.9883C102.9893,-48.9098 102.9893,-39.1714 102.9893,-30.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.4894,-30.3038 102.9893,-20.3039 99.4894,-30.3039 106.4894,-30.3038\"/>\n",
       "</g>\n",
       "<!-- 4831703896 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4831703896</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"152.1305,-132 53.848,-132 53.848,-112 152.1305,-112 152.1305,-132\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.9893\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ThAddBackward</text>\n",
       "</g>\n",
       "<!-- 4831703896&#45;&gt;4831703728 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4831703896&#45;&gt;4831703728</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M102.9893,-111.9883C102.9893,-104.9098 102.9893,-95.1714 102.9893,-86.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.4894,-86.3038 102.9893,-76.3039 99.4894,-86.3039 106.4894,-86.3038\"/>\n",
       "</g>\n",
       "<!-- 4831703560 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4831703560</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"97.9678,-300 .0107,-300 .0107,-280 97.9678,-280 97.9678,-300\"/>\n",
       "<text text-anchor=\"middle\" x=\"48.9893\" y=\"-286.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ThMulBackward</text>\n",
       "</g>\n",
       "<!-- 4831703560&#45;&gt;4831703896 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4831703560&#45;&gt;4831703896</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M52.2055,-279.9939C60.8889,-252.9789 84.9341,-178.1717 96.5474,-142.0414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.9935,-142.7574 99.7216,-132.166 93.3293,-140.6153 99.9935,-142.7574\"/>\n",
       "</g>\n",
       "<!-- 4831704400 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4831704400</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"178.9893,-368 124.9893,-368 124.9893,-336 178.9893,-336 178.9893,-368\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.9893\" y=\"-354.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">y</text>\n",
       "<text text-anchor=\"middle\" x=\"151.9893\" y=\"-342.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3, 1)</text>\n",
       "</g>\n",
       "<!-- 4831704400&#45;&gt;4831703560 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4831704400&#45;&gt;4831703560</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125.1961,-335.8721C109.5979,-326.4829 90.0368,-314.7082 74.6385,-305.4393\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"76.034,-302.1942 65.6614,-300.0357 72.424,-308.1916 76.034,-302.1942\"/>\n",
       "</g>\n",
       "<!-- 4831704680 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>4831704680</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"200.1268,-300 115.8518,-300 115.8518,-280 200.1268,-280 200.1268,-300\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.9893\" y=\"-286.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">NegBackward</text>\n",
       "</g>\n",
       "<!-- 4831704400&#45;&gt;4831704680 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>4831704400&#45;&gt;4831704680</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M153.5657,-335.7102C154.3104,-328.0144 155.2076,-318.744 155.9987,-310.5691\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"159.5079,-310.6419 156.9875,-300.3512 152.5405,-309.9676 159.5079,-310.6419\"/>\n",
       "</g>\n",
       "<!-- 4831704232 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4831704232</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101.9639,-362 18.0146,-362 18.0146,-342 101.9639,-342 101.9639,-362\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.9893\" y=\"-348.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogBackward</text>\n",
       "</g>\n",
       "<!-- 4831704232&#45;&gt;4831703560 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4831704232&#45;&gt;4831703560</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M58.1728,-341.762C56.6511,-333.185 54.4331,-320.6836 52.5581,-310.1154\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"55.9651,-309.2823 50.7719,-300.0475 49.0727,-310.5052 55.9651,-309.2823\"/>\n",
       "</g>\n",
       "<!-- 4831704456 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4831704456</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"199.8101,-424 94.1685,-424 94.1685,-404 199.8101,-404 199.8101,-424\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.9893\" y=\"-410.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 4831704456&#45;&gt;4831704232 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4831704456&#45;&gt;4831704232</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M132.623,-403.762C118.9405,-394.0113 98.1411,-379.1887 82.3012,-367.9005\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.2631,-365.0008 74.0882,-362.0475 80.2006,-370.7014 84.2631,-365.0008\"/>\n",
       "</g>\n",
       "<!-- 4831705016 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>4831705016</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"292.1268,-362 207.8518,-362 207.8518,-342 292.1268,-342 292.1268,-362\"/>\n",
       "<text text-anchor=\"middle\" x=\"249.9893\" y=\"-348.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">NegBackward</text>\n",
       "</g>\n",
       "<!-- 4831704456&#45;&gt;4831705016 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>4831704456&#45;&gt;4831705016</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M163.9976,-403.762C180.4963,-393.8307 205.7359,-378.638 224.6096,-367.2771\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"226.5348,-370.2034 233.2974,-362.0475 222.9248,-364.2061 226.5348,-370.2034\"/>\n",
       "</g>\n",
       "<!-- 4831704512 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4831704512</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"205.1363,-480 88.8422,-480 88.8422,-460 205.1363,-460 205.1363,-480\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.9893\" y=\"-466.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ClampMinBackward</text>\n",
       "</g>\n",
       "<!-- 4831704512&#45;&gt;4831704456 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4831704512&#45;&gt;4831704456</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.9893,-459.9883C146.9893,-452.9098 146.9893,-443.1714 146.9893,-434.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.4894,-434.3038 146.9893,-424.3039 143.4894,-434.3039 150.4894,-434.3038\"/>\n",
       "</g>\n",
       "<!-- 4831704624 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>4831704624</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"196.1305,-536 97.848,-536 97.848,-516 196.1305,-516 196.1305,-536\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.9893\" y=\"-522.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ThAddBackward</text>\n",
       "</g>\n",
       "<!-- 4831704624&#45;&gt;4831704512 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>4831704624&#45;&gt;4831704512</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.9893,-515.9883C146.9893,-508.9098 146.9893,-499.1714 146.9893,-490.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.4894,-490.3038 146.9893,-480.3039 143.4894,-490.3039 150.4894,-490.3038\"/>\n",
       "</g>\n",
       "<!-- 4831704736 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>4831704736</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"197.7905,-592 96.188,-592 96.188,-572 197.7905,-572 197.7905,-592\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.9893\" y=\"-578.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ExpandBackward</text>\n",
       "</g>\n",
       "<!-- 4831704736&#45;&gt;4831704624 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4831704736&#45;&gt;4831704624</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.9893,-571.9883C146.9893,-564.9098 146.9893,-555.1714 146.9893,-546.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.4894,-546.3038 146.9893,-536.3039 143.4894,-546.3039 150.4894,-546.3038\"/>\n",
       "</g>\n",
       "<!-- 4831704848 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>4831704848</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"189.1384,-648 104.8401,-648 104.8401,-628 189.1384,-628 189.1384,-648\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.9893\" y=\"-634.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 4831704848&#45;&gt;4831704736 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>4831704848&#45;&gt;4831704736</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.9893,-627.9883C146.9893,-620.9098 146.9893,-611.1714 146.9893,-602.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.4894,-602.3038 146.9893,-592.3039 143.4894,-602.3039 150.4894,-602.3038\"/>\n",
       "</g>\n",
       "<!-- 4831704960 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>4831704960</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"173.9893,-716 119.9893,-716 119.9893,-684 173.9893,-684 173.9893,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.9893\" y=\"-702.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">X</text>\n",
       "<text text-anchor=\"middle\" x=\"146.9893\" y=\"-690.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3, 2)</text>\n",
       "</g>\n",
       "<!-- 4831704960&#45;&gt;4831704848 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>4831704960&#45;&gt;4831704848</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M146.9893,-683.7102C146.9893,-676.0144 146.9893,-666.744 146.9893,-658.5691\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.4894,-658.3512 146.9893,-648.3512 143.4894,-658.3513 150.4894,-658.3512\"/>\n",
       "</g>\n",
       "<!-- 4831704120 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>4831704120</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"206.9678,-188 109.0107,-188 109.0107,-168 206.9678,-168 206.9678,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.9893\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ThMulBackward</text>\n",
       "</g>\n",
       "<!-- 4831704120&#45;&gt;4831703896 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>4831704120&#45;&gt;4831703896</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M148.1564,-167.9883C140.4318,-160.1233 129.4821,-148.9745 120.2984,-139.6239\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.6133,-136.9859 113.1091,-132.3039 117.6192,-141.8909 122.6133,-136.9859\"/>\n",
       "</g>\n",
       "<!-- 4831704344 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>4831704344</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"200.8004,-244 115.1781,-244 115.1781,-224 200.8004,-224 200.8004,-244\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.9893\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward</text>\n",
       "</g>\n",
       "<!-- 4831704344&#45;&gt;4831704120 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>4831704344&#45;&gt;4831704120</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M157.9893,-223.9883C157.9893,-216.9098 157.9893,-207.1714 157.9893,-198.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.4894,-198.3038 157.9893,-188.3039 154.4894,-198.3039 161.4894,-198.3038\"/>\n",
       "</g>\n",
       "<!-- 4831704680&#45;&gt;4831704344 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>4831704680&#45;&gt;4831704344</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M157.9893,-279.9883C157.9893,-272.9098 157.9893,-263.1714 157.9893,-254.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.4894,-254.3038 157.9893,-244.3039 154.4894,-254.3039 161.4894,-254.3038\"/>\n",
       "</g>\n",
       "<!-- 4831704288 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>4831704288</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"302.9639,-244 219.0146,-244 219.0146,-224 302.9639,-224 302.9639,-244\"/>\n",
       "<text text-anchor=\"middle\" x=\"260.9893\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">LogBackward</text>\n",
       "</g>\n",
       "<!-- 4831704288&#45;&gt;4831704120 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>4831704288&#45;&gt;4831704120</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M242.5749,-223.9883C226.7084,-215.3619 203.576,-202.785 185.511,-192.9633\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"187.0487,-189.8155 176.5914,-188.1138 183.7051,-195.9653 187.0487,-189.8155\"/>\n",
       "</g>\n",
       "<!-- 4831704792 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>4831704792</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"303.8004,-300 218.1781,-300 218.1781,-280 303.8004,-280 303.8004,-300\"/>\n",
       "<text text-anchor=\"middle\" x=\"260.9893\" y=\"-286.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward</text>\n",
       "</g>\n",
       "<!-- 4831704792&#45;&gt;4831704288 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>4831704792&#45;&gt;4831704288</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M260.9893,-279.9883C260.9893,-272.9098 260.9893,-263.1714 260.9893,-254.4779\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"264.4894,-254.3038 260.9893,-244.3039 257.4894,-254.3039 264.4894,-254.3038\"/>\n",
       "</g>\n",
       "<!-- 4831705016&#45;&gt;4831704792 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>4831705016&#45;&gt;4831704792</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M251.8057,-341.762C253.3274,-333.185 255.5454,-320.6836 257.4204,-310.1154\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.9058,-310.5052 259.2066,-300.0475 254.0135,-309.2823 260.9058,-310.5052\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x111973ba8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "\n",
    "X = torch.tensor([[1., 1.], [-1., 5.], [-1., -1.]], requires_grad=True)\n",
    "y = torch.tensor([[1.],\n",
    "                  [1.],\n",
    "                  [1.]], requires_grad=True)\n",
    "W = torch.tensor([[2.], [3.]])\n",
    "b = torch.tensor([[-2.]])\n",
    "print('shapes X, y, W, b', X.shape, y.shape, W.shape, b.shape)\n",
    "### YOUR CODE HERE\n",
    "\n",
    "hin = (X@W) + b\n",
    "hout = hin.clamp(min=0)\n",
    "\n",
    "print('hin', hin)\n",
    "print('hout', hout)\n",
    "lg = torch.sigmoid(hout)\n",
    "print(\"lg shape\", lg.shape)\n",
    "loss_all = y * torch.log(lg)  + (1.-y) * torch.log(1.-lg)\n",
    "\n",
    "print('shapes', hin.shape, hout.shape, lg.shape, loss_all.shape)\n",
    "loss = -torch.sum(loss_all)\n",
    "print('loss', loss)\n",
    "hin.retain_grad()\n",
    "hout.retain_grad()\n",
    "loss.backward()\n",
    "print('Grad y', y.grad)\n",
    "print('Grad X\\n', X.grad)\n",
    "print('hout grad\\n', hout.grad)\n",
    "print('hin grad\\n', hin.grad)\n",
    "\n",
    "\n",
    "print('The hard way')\n",
    "d_hout = -(1. - torch.sigmoid(hout))\n",
    "print('d_hout.shape', d_hout.shape)\n",
    "D_relu = (hin>0).float()\n",
    "D_relu[D_relu<0] = 0\n",
    "d_hin = d_hout * (hin>0).float() #  do not need to create a matrix here - we know what it does\n",
    "print('d_hout', d_hout)\n",
    "print('d_hin', d_hin)\n",
    "d_X = d_hin @ torch.transpose(W, 1, 0)\n",
    "print('d_X:\\n', d_X)\n",
    "\n",
    "### END CODE\n",
    "# rename if your loss is not named loss\n",
    "make_dot(loss, params={'X': X, 'y': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
