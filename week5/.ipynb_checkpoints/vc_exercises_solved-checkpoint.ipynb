{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 1: Generalization\n",
    "\n",
    "\n",
    "## Questions:\n",
    "The Hoeffding bound gives us the following guarantee:\n",
    "$$\n",
    "\\Pr[|E_{in}-E_{out}| > \\varepsilon] \\leq 2Me^{-2\\varepsilon^2 n},\n",
    "$$\n",
    "where the probability is over the random choice of the sample.\n",
    "\n",
    "<b>Question 1: </b> \n",
    "Does the Hoeffding bound give any meaningful bounds on $E_{in}$ and $E_{out}$ for the perceptron learning model?  \n",
    "\n",
    "\n",
    "### SOLUTION MATH\n",
    "No since the perceptron learning algorithm has infinitely many hypotheses.\n",
    "\n",
    "### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: Break Points and Growth Functions \n",
    "\n",
    "-   Is there always a break point for a finite hypothesis set of $n$\n",
    "    hypotheses? If so, can you give a an upper bound? What is the growth\n",
    "    function?\n",
    "\n",
    "-   Does the set of all functions have a break point? What is its growth\n",
    "    function?\n",
    "\n",
    "-   What is the (smallest) break point for the hypothesis set consisting\n",
    "    of circles centered around $(0,0)$? For a given circle the\n",
    "    hypothesis returns $1$ for points inside the circle and $-1$ for\n",
    "    points outside. What is the growth function?\n",
    "\n",
    "-   What if we move to centered balls in the 3-dimensional space\n",
    "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n",
    "    ${{\\mathbb R}}^d$ (hyperspheres)?\n",
    "\n",
    "-  Show that the growth function for a singleton hypothesis class $H = \\{h\\}$ is 1\n",
    "\n",
    "### SOLUTION MATH\n",
    "1. Since we have $n$ hypotheses, the growth function is capped at $n$. Therefore it must have a break point of at most $\\lfloor \\lg n \\rfloor + 1$.\n",
    "\n",
    "2. It does not have a break point. Its growth function is $m(n)=2^n$ since all functions can create all dichotomies.\n",
    "\n",
    "3. The break point is at 2 since for two points of distinct distances, it is impossible to construct the dichotomy having $1$ on the point furthest from the origin and $-1$ on the point closest. The growth function is the same as positive rays in 1d (sort by distances), hence $m(n)=1+n$.\n",
    "\n",
    "4. The same since all that matters is the distance to the origin.\n",
    "\n",
    "5. For a single point, it is impossible to create both dichotomies since $h$ only gives one fixed output on the point.\n",
    "\n",
    "### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3: VC Dimension \n",
    "\n",
    "-   Does VC Dimension depend on the learning algorithm or the actual\n",
    "    data set given?\n",
    "\n",
    "-   Does VC Dimension depend on the probability distribution generating\n",
    "    the data (not the labels)?\n",
    "\n",
    "-   If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n",
    "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n",
    "\n",
    "-   Can you give an upper bound on the VC dimension of a finite set of\n",
    "    $M$ hypotheses?\n",
    "\n",
    "-   What is the VC Dimension for the hypothesis set consisting of\n",
    "    circles centered around 0?\n",
    "\n",
    "-   What if we move to balls (3d)? or in general d dimensions\n",
    "    (hypershperes)?\n",
    "\n",
    "-   What is the maximal VC dimension possible of the intersection of\n",
    "    hypothesis spaces $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n",
    "    dimension $v_1,\\dots,v_n$.\n",
    "\n",
    "-   As previous question, instead what is the minimal VC dimension of\n",
    "    the union of the hypothesis spaces from the previous question\n",
    "\n",
    "-   Show that the VC dimension the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4,\n",
    "    i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 can not.\n",
    "    \n",
    "### SOLUTION MATH\n",
    "1. No\n",
    "2. No\n",
    "3. yes\n",
    "4. yes, $\\lfloor log M \\rfloor$. To see this, notice that for VC dimension $d$, one must be able to shatter a set of $d$ points, i.e. $M \\geq 2^d$.\n",
    "5. same as positive rays in 1d, namely 1. sort by distance -1 +1 is impossible.\n",
    "6. The same\n",
    "7. The minimum of the VC dimensions since the intersection is contained in all.\n",
    "8. The largest of the VC dimensions since you always keep at least the largest and all others might be subset of it.\n",
    "9. To prove at least 4, pick points (0,1), (0,-1), (1,0), (-1,0). Not hard to see that all dichotomies can be obtained. To prove less than 5, consider any set of 5 points in the plane. Computing their bounding box $B$. All the 5 points are containing in $B$, and to construct the \"full\" dichotomy, one must use a rectangle containing at least $B$. Now consider the dichotomy that contains all points on the boundary of the bounding box but none of the points inside. That dichotomy is impossible if the box $B$ has a point in its interior. Finally, if all 5 points lie on the boundary of the bounding box, there are two that lie on the same side of the bounding box. Any dichotomy involving all but one of those two is impossible.\n",
    "### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n",
    "Consider the input space $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$ (with the first coordinate being the constant 1). Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^{d+1} \\}$ corresponding to the perceptron is $d+1$.\n",
    "\n",
    "We need to show \n",
    "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n",
    "2. That no data set of size d+2 can be shattered by hyperplanes\n",
    "\n",
    "We will give a few more hints than the book does.\n",
    "### Shattering d+1 points\n",
    "As the book hints you must create an \"easy\" data set that you store in matrix $X$. \n",
    "\n",
    "**Hint:** We suggest you consider as a data matrix, the $(d+1) \\times (d+1)$ matrix $X$ whose first column is all-1s (required since $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$) and where the lower $d \\times d$ corner of the matrix is the $d \\times d$ identity matrix.\n",
    "\n",
    "Show that you can construct any dichotomy $y \\in \\{-1,+1\\}^{d+1}$ using some $h \\in \\mathcal{H}$ and the data matrix $X$ defined above.\n",
    "\n",
    "### SOLUTION MATH\n",
    "Given a dichotomy $y \\in \\{-1,+1\\}^{d+1}$, we pick the hypothesis $w$ such that $w_1 = 0.1y_1$ and $w_i = y_i$ for $i>1$. Observe that for the first row of $X$ (data item $x_1$), we have $w^\\intercal x_1 = w_1 = 0.1 y_1$, hence $\\textrm{sign}(w^\\intercal x_1) = y_1$. For $i>1$, notice that $w^\\intercal x_i = w_1 + w_i = 0.1y_1 + y_i$. Since $y_1$ is multiplied by the factor $0.1$, we still have $\\textrm{sign}(w^\\intercal x_i) = y_i$. \n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "### No Shattering of d+2 points.\n",
    "Must show that for any d+2 points we must prove there is a  dichotomy hyperplanes can not capture.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Consider an arbitrary set of d+2 points $x_1,\\dots, x_{d+2}$ of dimension (d+1) and think of them as vectors in $\\{1\\} \\times \\mathbb{R}^d \\subset \\mathbb{R}^{d+1}$.\n",
    "Since we have more vectors than dimensions the vectors must be linearly dependent.\n",
    "\n",
    "i.e. \n",
    "$$\n",
    "x_j = \\sum_{i\\neq j} a_i x_i\n",
    "$$\n",
    "Since $x_j$ is determined by the other data points then so is $w^\\intercal x_j$ for any $w$. This means the classification on point $x_j$ is dictated by the classification of the other data points and thus cannot freely be chosen.\n",
    "i.e.\n",
    "$$\n",
    "w^\\intercal x_j = w^\\intercal \\sum_{i\\neq j} a_i x_i =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Define an impossible dichotomy as follows. \n",
    "$$\n",
    "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n",
    "$$\n",
    "Show this dichotomy is impossible!\n",
    "\n",
    "### SOLUTION MATH\n",
    "We have \n",
    "$$\n",
    "w^\\intercal x_j =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Furthermore, we must have $\\textrm{sign}(a_i) = y_i$ for $i \\neq j$ and $\\textrm{sign}(w^\\intercal x_i) = y_i$ for $i \\neq j$. This means that $a_i w^\\intercal x_i$ is positive for $i \\neq j$. Hence $w^\\intercal x_j > 0$ and we cannot have $\\textrm{sign}(w^\\intercal x_j) = -1$.\n",
    "### END SOLUTION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 5:  Book Exercises\n",
    "## Exercise 1.11 and 1.12 in the Book \n",
    "(Not problems but exercises inside the text. page 25, 26\n",
    "\n",
    "### SOLUTION MATH\n",
    "1.11\n",
    "a. No. The training data might be such that it contains most examples with the most unlikely label.\n",
    "\n",
    "b. Yes. It might be the case that $p$ is very small, say 0.1, and yet the sample has only points with label +1.\n",
    "\n",
    "c. It equals the probability that $B(n,p)>n/2$ where $n=25$. We can e.g. use Hoeffding to get a bound on this. Let $X \\sim B(n,p)/n$. We see that $\\Pr[|X-p|>1/2-p] \\leq 2\\exp(-2(1/2-p)^2 n) = 2\\exp(-2 \\cdot 0.4^2 \\cdot 25) < 0.00068$. Hence $S$ produces a better hypothesis with probability at least $1-0.00068$.\n",
    "\n",
    "d. No as $S$ outputs the maximum likelihood estimate.\n",
    "\n",
    "1.12\n",
    "a. is not possible as $f$ is unknown.\n",
    "\n",
    "b. not possible. \n",
    "\n",
    "c. Is possible by using a small hypothesis set. If you find a useful hypothesis (low in sample error), you produce that g. Otherwise you report failed.\n",
    "\n",
    "### END SOLUTION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: Book Problem 2.18 In short\n",
    "Define\n",
    "$$\n",
    "\\mathcal{H}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n",
    "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}\n",
    "$$ \n",
    "\n",
    "Show that the VC dimension of ${{\\mathcal H}}$ is infinite (even though there is only one parameter!)\n",
    "\n",
    "Hint: Use the points set\n",
    "$x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to implement any dichotomy $y_1,\\dots,y_N \\in \\{-1, +1\\}^N$ (find $\\alpha$ that works).\n",
    "You can safely assume $\\alpha >0$.\n",
    "\n",
    "### SOLUTION MATH\n",
    "Map $y$ to $\\alpha$ as follows. define \n",
    "$$\n",
    "f(y) = \\begin{cases}\n",
    "1 \\textrm{ if } y = -1\\\\\n",
    "2 \\textrm{ if } y = +1\n",
    "\\end{cases}\n",
    "$$\n",
    "set $\\alpha$ to the number defined by concatenating the mapped digits $0.f(y_1)f(y_2)\\dots f(y_N)$\n",
    "This means that for $y = [-1, +1, -1]$, $\\alpha = 0.121$.\n",
    "\n",
    "### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
